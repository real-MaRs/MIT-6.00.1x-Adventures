# MIT-6.00.1x-Adventures
MIT's Introduction to Computer Science and Programming in Python Course

# 3/15/17

Week 6 - Algorithmic Complexity

  6.1 - Computational Complexity
    
    6.1.1 - Program Efficiency: Efficiency of a program becomes more important as data becomes larger and larger. A program has
            separate time and space efficiency. There will usually be a tradeoff between the two. Different inputs could change how the
            program runs. The worst case is generally the most valuable to know when a piece of code runs. The complexity of a code is
            the orders of growth, how the code runs as the input size gets bigger.
            
            Types of Orders of Growth:
              Constant: Always takes the same amount of time, in general
              Linear: Grows with the size of the input, double the size of input means double the time it takes
              Quadratic: Grows by squared/ ^2
              Logarithmic: Grows with the log of the size of the problem
              N log n: Not as bad as quadratic and a little better than linear
              Exponential: grows exponentially with the input

    6.1.2 - Big Oh Notation: Big Oh notation measures the upper bound on the asymptotic growth, often called order of growth. Big Oh, O()
            is used to describe the worst case. With Big Oh, you ignore the constants and multiplicative factors and focus on the dominant
            term.
            
    6.1.3 - Complexity Classes: Examples of Logarithmic complexity would be bisection search or binary search of a list. An example of 
            linear complexity would be search through a list to find an element.
            
    6.1.4 - Analyzing Complexity: The most common type of polynomial algorithms are quadratic. Commonly occurs with nest loops or recursive
            calls. 
            
    6.1.5 - More Analyzing Complexity: Exponential algorithms are the most expensive type of algorithm. A common trait of these would be
            functions that has multiple recursive calls of itself, such as Towers of Hanoi.
            
    6.1.6 - Recursion Complexity: It is important to look over what exact the code is looping through and how your input affects how many 
            times it loops. Big Oh is used to compare efficieny of algorithms. The lower the order of growth, the better.


#3/23/17

6.2 - Searching and Sorting Algorithms

  6.2.1 - Search Algorithms: They are algorithms for finding an item or group of items within a collection of items. There are different 
          kinds of search algorithms. The linear search, also known as the brute force search, can search for an item in an unsorted list.
          The bisection search is alot faster but the items must be in order.
          
  6.2.2 - Bisection Search: Only works on sorted elements. Check the middle of the list and check if the element is equal, greater, or smaller
          and cut the list accordingly. The complexity is O(log n). It is better to sort and then search than to just search.
          
  6.2.3 - Bogo Sort: Randomly assigns elements in a list and check of they are in order, if they are not then they are randomly assigned
          again. It is very inefficient and the complexity is O(?).
          
  6.2.4 - Bubble Sort: Compares consecutive pairs of elements and swaps the elements if the second is smaller than the first. When reach
          end of list, go back to the start. Sorting stops when no more swaps have been made. The complexityt is O(n^2)
          
  6.2.5 - Selection Sort: First step, you find the smallest element and swap it to index 0. At each subsequent step, get the smallest
          value in the remaining sublist and swap it with the element at index 1. The left side of the list will be sorted.
          
  6.2.6 - Merge Sort: 
